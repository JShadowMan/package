Go - 分布式系统
-------------

互联网时代的后台服务由大量的分布式系统构成，任何单一后台服务器节点的故障都不会导致整个系统的停机。


### 分布式Id生成器

有时候我们需要能够生成不断增大又不会重复的ID，以支持业务中的高并发场景。Twitter的Snowflake算法就是这种情景下一个典型解法。

Snowflake算法使用64位整数作为Id，其中首位不使用，随后41位表示毫秒级时间戳（timestamp），随后5位表示数据中心Id（datacenter_id），在之后5位表示机器Id（worker_id），最后12位为循环自增Id（sequence_id）。该算法理论上可以使用69年，并且可以在1毫秒内生成4096个Id（每秒409.6万个Id），支持32个数据中心，每个数据中心支持32台机器（共1024台机器）。

__需要注意的是：Twitter官方已经声明该方法并不是十分安全，问题集中在不同机器的毫秒级时间戳上。__

典型的Snowflake算法的实现可以参考：`github.com/bwmarrin/snowflake`

#### worker_id的分配

在4个字段中，timestamp和sequence_id都是程序中生成。唯独datacenter_id和worker_id是需要在部署节点就确定的，而一旦程序启动就不可更改。一直比较简单的思路是引入第三方服务来提供唯一的worker_id，比如使用Ip在Mysql插入以获得唯一值。更方便的方法是直接使用Ip地址的低16位作为worker_id（但是需要注意跨机房情况下Ip地址相同问题）。再就是可以在部署过程中，直接由部署脚本完成worker_id的生成。

#### sonyflake

sonyflake为Sony公司的一个开源项目，基本思路和snowflake相同，只是在不同字段上的定义有所不同。
 * `timestamp`：39位10毫秒级时间戳，可用174年
 * `sequence_id`：8位自增Id
 * `machine_id`：16位机器Id


### 分布式锁

单机环境下如果需要并发或者并行修改全局变量，可以对修改行为加锁以创建临界区来保证原子性。在另外的一些场景下可能是需要独占锁，这在Go中可以通过大小为1的通道来模拟。
```go
package lock

type Lock struct { c chan struct{} }

func (l *Lock) Lock() bool {
	select {
	case <-l.c:
		return true
	default:
        return false
    }
}

func (l *Lock) Unlock() {
    l.c <- struct{}{}
}
```

#### 分布式场景下的锁方案

简单情况下，我们可以使用Redis提供的`setnx`命令（只有在键不存在的情况下才可以设置成功）。`setnx`方案很适合在高并发场景下，用来争抢一些“唯一”的资源。

还可以使用基于Zookeeper的Sequence节点写入的原子性来实现分布式锁。多个并发任务同时在目标节点插入自己的值，通过监听API在值被修改时检查是否与自己设置的值相同来判断是否加锁成功。这种基于zk的分布式锁比较适合分布式任务调度场景，但不适合高频次持锁时间短的抢锁场景。__基于强一致性协议的锁适用于粗粒度的加锁操作（粗粒度指锁占用时间长）__

类似于Zookeeper，我们也可以基于etcd实现锁，基本原理是尝试在一个路径写入自己的值，如果写入成功则说明加锁成功。失败后监视这个路径，并在发生事件时重试加锁。__etcdv3的官方API已经提供了可直接使用的锁API__

#### 如何选择合适的分布式锁方案

 * 如果业务还是单机可以搞定的量级，那就按照需求使用任意的单机锁方案就可以。
 * 如果到了分布式业务阶段，但是业务规模不大，使用哪种分布式锁方案都差不多。
 * 如果业务到了一定阶段且锁需要在任何恶劣情况下都不允许数据丢失，那就不要使用Redis的setnx方案。
 * 如果对锁的可靠性要求极高，那就只能使用etcd或者zk这种通过强一致性协议保证数据可靠性的方案了。
    * 强一致性带来的可靠背后往往是较低的吞吐量和较高的延迟。


### 延时任务系统

在数据规模比较小时，可以通过数据库配合轮询来实现。一旦上了规模，一般会有两种思路来解决这个问题：
 * 实现一套类似crontab的分布式定时任务管理系统
 * 实现一个支持定时发送消息的消息队列
两种思路其实都是对定时器的应用，业界常见的解法有时间堆和时间轮：
 * 时间堆：一般使用小定堆实现，小顶堆其实就是一种特殊的二叉树
    * 小顶堆的性质是：父节点都子节点都要小
    * 如果堆顶元素比当前时间大，说明没有任务需要处理
    * 如果堆顶元素小于当前时间，说明有任务需要处理，这时候进行正常的弹出和调整操作就可以
    * Go中采用的时间堆的方式实现定时器，不过是更加扁平一点的四叉堆
 * 时间轮：从结构上和散列表很像，简单地说就是将定时时间%时间轮大小，并在链表上存储冲突元素


### 分布式搜索引擎

Elasticsearch是开源分布式搜索引擎的霸主，其依赖于Lucene实现，在部署和运维方面做了很多优化。Elasticsearch之所以在搜索场景表现出色，就是其使用倒排列表的特性。

#### 数据同步

在实际应用中，一般很少直接向搜索引擎中写入数据，更常见的方式是将MySQL或其他关系型数据库中的数据同步到搜索引擎中。常见的同步方案有基于时间戳的增量数据同步和基于binlog订阅的数据同步方案。

基于binlog的数据统同步业界使用较多的是阿里开源的Canal，Canal将自己伪装成MySQL的从库，然后对同步的binlog进行解析，并将其解析成如JSON格式发送到消息队列。


### 负载均衡

简单的负载均衡方案有：1、按顺序挑。2、随机挑一台。3、根据某种权重方案进行排序，选择权重最大的或者最小的那个。


### 分布式配置管理

虽然目前已经有一些优雅的重启方案，但是在某些场景下我们可以尽量避免采用上下线的方式，对线上的程序做一些修改。比较典型的修改内容就是程序的配置项。

我们可以通过etcd实现一个简单的配置读取和动态更新流程。通过Get方法获取配置，并通过监听功能实现动态更新。__需要注意的是在更新配置时的一系列操作时不具有原子性的，在一个业务处理中很可能在执行中发生配置更新，导致前后逻辑不统一的问题。__

#### 配置膨胀

随着业务的发展，配置系统本身所承受的压力也会越来越大，配置文件可能会成千上万。如果客户端也成千上万，可能会导致配置系统无法承受瞬时大量的QPS，所有 __还需要在客户端侧实现缓存机制__。

#### 配置版本管理

在配置管理过程中，难免会出现用户误操作的情况。为了快速止损，最快且最有效的方案是进行版本管理，并支持按版本回退。

__同时非常重要的一点：需要有配置的审批流程__

#### 客户端容错

当配置中心宕机时，客户端也需要有一定的容错能力，最起码在配置服务宕机期间，业务能正常运转，哪怕是配置信息不够新。常见的做法是在本地磁盘上缓存一份配置。

__引入缓存之后务必需要考虑数据一致性的问题。__


### 分布式爬虫

可以使用colly来实现单机爬虫，通过结合nats（高性能分布式消息队列，适用于高并发搞吞吐量的消息分发场景）实现分布式爬虫。
